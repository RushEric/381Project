{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4080f9",
   "metadata": {},
   "source": [
    "# CSCI 381/780 (Fall 2023) - Project 1\n",
    "\n",
    "**Due Date: Monday, October 2 by 4 PM**\n",
    "\n",
    "## Description\n",
    "In this project you will construct machine learning models on a dataset comprised of sociodemographic and financial data from a German bank. The machine learning task will be binary classification of people's creditworthiness for a bank loan.\n",
    "\n",
    "## Instructions\n",
    "1. In this project you will write code to construct machine learning models using various partitions (see the figure below) of the census dataset and write responses to questions concerning the performance of said models. Please complete all sections below, adding new *Code* or *Markdown* cells as appropriate to answer the questions.\n",
    "2. There are many Scikit-learn functions that leverage randomness to generate results. For these functions, a pseudorandom generator can be initialized using a seed value by passing the parameter `random_state=XXX`, where `XXX` is some number between 1 and 2^31-1. For each of these functions, **you will utilize your CUNY ID number** to initialize the function. Functions include:\n",
    "- `StratifiedShuffleSplit`\n",
    "- `RandomForestClassifier`\n",
    "- `RandomizedSearchCV`\n",
    "- `mutual_info_classif`\n",
    "3. You will **work independently** on the project. Please make use of the *Python Data Science Reference Materials* posted on Blackboard or *come to office hours* should you need further assistance.\n",
    "4. You will submit a single Jupyter notebook containing all code and written responses via Blackboard by the due date listed above. \n",
    "\n",
    "<img src=\"project-1-data-folds.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "## Grading\n",
    "\n",
    "### Running Code\n",
    "Your Jupyter notebook must be able to run from start to finish **without error**. Please turn any cell that contains scratch work or other non-executable items to *Raw*. **Notebooks that cannot run to completion will receive a grade of 0**.\n",
    "\n",
    "### Holdout Set Evaluation\n",
    "Your final models will be evaluated against a holdout set. You model performances with respect to AUC on this set must be *comparable* (e.g., within 5%) of those reported in Part 6.\n",
    "\n",
    "### Rubric\n",
    "\n",
    "|**Part**|0|1|2|3|4|5|6|7|**Total**|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**%**|20|10|10|15|10|10|15|10|100|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2a56d",
   "metadata": {},
   "source": [
    "## Part 0: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc78b24",
   "metadata": {},
   "source": [
    "Set the variable `CREDIT_FILE` to the **full path** to the credit dataset (**credit_dataset.xlsx**) on your system. Load the file into a dataframe, then:\n",
    "1. Determine the number and types (*numeric*, *categorical*, *ordinal*) for each feature in the dataset.\n",
    "2. Determine if there are any missing/null values for any features. \n",
    "3. Determine the number of classes and their prevalence in the dataset. Are the classes balanced?\n",
    "4. The **features** of this must be encoded prior to being used to construct machine learning models. Create **three** encodings for the dataset:\n",
    "  1. for any categorical variables, utilize an **ordinal** encoding.\n",
    "  2. for any categorical variables, utilize a **one-hot** encoding.\n",
    "  3. encode variables as **appropriate** - ordinal, one-hot, etc. For any ordinal variables, use an explicit ordering by using the `categories` argument for the `OrdinalEncoder` class in Scikit-learn.\n",
    "5. The **labels** must be encoded prior to being used to construct machine learning models. Encode \"good\" as 1 (the *positive* class) and \"bad\" as 0 (the *negative* class). The technique in 4C above can be used.\n",
    "6. Perform a **stratified split** of the data, **for each encoding from step 4**, into training/validation/test sets, 80%/10%/10%. \n",
    "7. Verify that the training/validation/test splits have the same prevalence as the original dataset.\n",
    "8. Standardize **only numeric features** in the training/validation/test splits (fit on the training, then transform the validation/test sets). You may find the class `ColumnTransformer` useful for this task, but be mindful of the column reordering that results. Use the standardized splits for the SVM models *only*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1a3c8",
   "metadata": {},
   "source": [
    "## Part 1: Train Initial Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c87b8",
   "metadata": {},
   "source": [
    "Using default hyperparameters:\n",
    "1. Construct **Naive Bayes (NB)**, **Support Vector Machine (SVM)** , and **Random Forest (RF)** models on the training sets for each encoding from step 4.\n",
    "2. Calculate the confusion matrix and report the following performance metrics on the **training set**:\n",
    "    *Accuracy*, *F1 Score*, *AUC*, *Sensitivity*, *Specificity*, *Precision*, and *NPV*. You can use the function `binary_metrics` for this purpose. Are any of the models underfitting the data? Is so, why?\n",
    "\n",
    "3. Calculate the same metrics by applying the trained model to the **validation set**. Compare and contrast the errors each model makes in terms of each class.\n",
    "4. Determine which dataset encoding yields the best performance on the **validation set**. Describe why you think this encoding performs the best. **Use only the best encoding for the remainder of the project**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peformance metric functions\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "#A list of keys for the dictionary returned by binary_metrics\n",
    "metric_keys = ['auc','f1','accuracy','sensitivity','specificity', 'precision', 'npv']\n",
    "\n",
    "def binary_metrics(y_true,y_pred,include_cm=True):\n",
    "    cm = confusion_matrix(y_true,y_pred,labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    #set AUC to NaN if all labels are of a single class\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        auc = roc_auc_score(y_true,y_pred) \n",
    "    else:\n",
    "        auc = float('nan')\n",
    "\n",
    "    #set F1 to NaN if all predictions are true negatives\n",
    "    if (fp + fn +tp) >0:\n",
    "        f1=f1_score(y_true,y_pred)\n",
    "    else:\n",
    "        f1 = float('nan')\n",
    "\n",
    "    #set metrics to NaN if there are zero divisors\n",
    "    sensitivity = tp/(tp+fn) if (tp+fn)!= 0 else float('nan')\n",
    "    specificity = tn/(tn+fp) if (tn+fp)!= 0 else float('nan')\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else float('nan')\n",
    "    npv = tn/(tn+fn) if (tn+fn) !=0 else float('nan')\n",
    "\n",
    "    if include_cm:\n",
    "        return {\n",
    "            'auc': auc,\n",
    "            'f1': f1,\n",
    "            'accuracy': (tp+tn)/np.sum(cm),\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'precision': precision,\n",
    "            'npv': npv,\n",
    "            'confusion_matrix': cm}\n",
    "    else:\n",
    "        return {\n",
    "            'auc': auc,\n",
    "            'f1': f1,\n",
    "            'accuracy': (tp+tn)/np.sum(cm),\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'precision': precision,\n",
    "            'npv': npv}\n",
    "\n",
    "#This wrapper can be used to return multiple performance metrics during cross-validation\n",
    "def binary_metrics_scorer(clf,X,y_true):\n",
    "    y_pred=clf.predict(X)\n",
    "    return binary_metrics(y_true,y_pred,include_cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784cab5",
   "metadata": {},
   "source": [
    "## Part 2: Cross-Validation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865e390",
   "metadata": {},
   "source": [
    "Split the **non-test data** (*training* + *validation* data) into **stratified 5-folds** for cross-validation purposes, then:\n",
    "1. Train NB, SVM, and RF models using 5-fold cross-validation.\n",
    "2. Report the mean and standard deviation of the performance metrics listed in Part 1.2 for each model. You may use the function `collate_cv_results` for this purpose.\n",
    "3. How does the performance of these models compare with those created in Part 1? Which models' performances are more consistent, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f31b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizes model performance results produced during cross-validation\n",
    "def collate_cv_results(cv_results,display=True):\n",
    "    cv_stats=dict()\n",
    "    for k in cv_results:\n",
    "        cv_stats[k+\"_mean\"]=np.mean(cv_results[k])\n",
    "        cv_stats[k+\"_std\"]=np.std(cv_results[k])\n",
    "        if display:\n",
    "            print(k,cv_stats[k+\"_mean\"],\"(\"+str(cv_stats[k+\"_std\"])+\")\")\n",
    "    return cv_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee068b",
   "metadata": {},
   "source": [
    "## Part 3: SVM Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ee07f",
   "metadata": {},
   "source": [
    "Utilizing the cross-validation dataset from Part 2, you will construct SVM models using the below strategies:\n",
    "1. Using `RandomizedSearchCV`, determine the best choice of hyperparameters after **30 trials**  using the following possible values:\n",
    "- *Kernel type*: Linear, polynomial, radial basis function\n",
    "- *Degree (will used by the polynomial kernel only)*: `[2,8]` inclusive.\n",
    "- *Box constraint (C)*: `loguniform(1e-3, 1e3)`\n",
    "- *Kernel width (gamma)*: `loguniform(1e-3, 1e3)`\n",
    "2. Report the time required to perform cross-validation via `RandomizedSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameters. You may use the function `collate_ht_results` for this purpose.\n",
    "3. Compare the performance of this model to the best SVM model constructed in Part 2. Which performs better?\n",
    "\n",
    "### Please Read!\n",
    "**For this part you will need to import the `loguniform` module from Scipy: ``from scipy.stats import loguniform``.**\n",
    "\n",
    "There are a few parameters for the `RandomizedSearchCV` function that should be set:\n",
    "- `scoring` - This controls the strategy to evaluate the performance of the cross-validated model on the test set, set it to `binary_metrics_scorer`.\n",
    "- `refit` - This will refit an estimator using the best found parameters on the whole dataset, set it to `\"auc\"`\n",
    "- `cv` - This will enable you to reuse your CV splits from Part 2.\n",
    "    `n_jobs` - Number of jobs to run in parallel, if you have more than one core on your device (you should), set this to as many as you'd like to use, or to `-1` if you want to use all available cores.\n",
    "- `return_train_score` - Setting this to `False` will reduce computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d86235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizes model performance results produced during hyperparameter tuning\n",
    "def collate_ht_results(ht_results,metric_keys=metric_keys,display=True):\n",
    "    ht_stats=dict()\n",
    "    for metric in metric_keys:\n",
    "        ht_stats[metric+\"_mean\"] = ht_results.cv_results_[\"mean_test_\"+metric][ht_results.best_index_]\n",
    "        ht_stats[metric+\"_std\"] = metric_std = ht_results.cv_results_[\"std_test_\"+metric][ht_results.best_index_]\n",
    "        if display:\n",
    "            print(\"test_\"+metric,ht_stats[metric+\"_mean\"],\"(\"+str(ht_stats[metric+\"_std\"])+\")\")\n",
    "    return ht_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2f1db",
   "metadata": {},
   "source": [
    "## Part 4: Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca639f6f",
   "metadata": {},
   "source": [
    "Utilizing the cross-validation dataset from Part 2, construct a Random Forest model:\n",
    "\n",
    "1. Using `RandomizedSearchCV`, determine the best choice of hyperparameters after **30 trials** using the following possible values:\n",
    "- *Split criterion*: Gini impurity, information gain (entropy)\n",
    "- *Maximum tree depth*:None, log<sub>2</sub>|cross-validation dataset|-1 (use `ceil` to round up to the nearest integer)\n",
    "- *Number of trees*: [10,20,...,100]\n",
    "2. Report the time required to perform cross-validation via `RandomizedSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameters. You may use the function `collate_ht_results` for this purpose.\n",
    "3. Compare the performance of this model to the best RF model constructed in Part 2. Which performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96964f7a",
   "metadata": {},
   "source": [
    "## Part 5: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6264831",
   "metadata": {},
   "source": [
    "Perform a stratified split of the **training data** into feature selection (1/8)/training (7/8) sets, then:\n",
    "1. Calculate feature importance on the feature selection subset via *Mutual Information (MI)*. Produce a bar graph showing each feature's importance and its standard deviation, as determined by MI, in descending order. Be sure to set the `random-state` keyword argument for the mutual information function `mutual_info_classif`.\n",
    "2. Choosing the **top 12 features**, train NB, SVM, and RF models on this new,smaller training set and test on the validation set.\n",
    "3. Choosing the **top 20 features**, train NB, SVM, and RF models on this new,smaller training set and test on the validation set.\n",
    "4. Compare the performance of the models with respect to the algorithm used (e.g., NB) and the number of features chosen (12, 20, and all features (those constructed in Part 1). How do they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a605ce",
   "metadata": {},
   "source": [
    "## Part 6: Final Models and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366abc9",
   "metadata": {},
   "source": [
    "1. Using the full training set (**feature selection + training + validation**), train NB, SVM, and RF models, then apply them to the test set. Your final NB, SVM, and RF models should be named `gnb_final`, `svm_final`, and `rf_final`.\n",
    "2. Create a bar chart of mean metrics from cross-validation, with standard deviation as an error bar for each model on the same plot. Use the NB model from Part 2, but the optimal models from Parts 3 and 4 for SVM and RF respectively.\n",
    "3. Plot Receiver Operating Characteristic (ROC) curves for each final model in a single plot.\n",
    "4. Which of the above metrics best illuminate the difference (if any) in model performance? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4ae38",
   "metadata": {},
   "source": [
    "## Part 7: Holdout Set Evaluation (Instructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e6dfe",
   "metadata": {},
   "source": [
    "Your final models `gnb_final`, `svm_final`, and `rf_final` will be evaluated on a holdout set by the instructor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
